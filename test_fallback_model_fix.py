#!/usr/bin/env python3
"""
Test for the fallback model fix - ensures that when using OpenRouter,
the fallback logic uses OpenRouter-compatible models, not OpenAI models.
"""

import os
import sys
import json
from unittest.mock import patch, MagicMock
from pathlib import Path

# Add the parent directory to sys.path so we can import the modules
sys.path.insert(0, str(Path(__file__).parent))

from utils.config import set_current_provider, get_current_provider
from utils.providers import get_model_for_task
from backend.jobs import start_deep_research_job


def test_fallback_model_selection():
    """Test that fallback uses provider-appropriate models"""
    print("Testing fallback model selection...")
    
    # Test 1: OpenAI provider should use OpenAI models
    set_current_provider('openai')
    current_provider = get_current_provider()
    deep_research_model = get_model_for_task('deep_research')
    chat_model = get_model_for_task('chat')
    
    print(f"OpenAI Provider:")
    print(f"  Current provider: {current_provider}")
    print(f"  Deep research model: {deep_research_model}")
    print(f"  Chat model: {chat_model}")
    
    assert current_provider == 'openai'
    assert deep_research_model == 'o4-mini-deep-research-2025-06-26'
    assert chat_model == 'gpt-4o-mini'
    
    # Test 2: OpenRouter provider should use OpenRouter models
    set_current_provider('openrouter')
    current_provider = get_current_provider()
    deep_research_model = get_model_for_task('deep_research')
    chat_model = get_model_for_task('chat')
    
    print(f"\nOpenRouter Provider:")
    print(f"  Current provider: {current_provider}")
    print(f"  Deep research model: {deep_research_model}")
    print(f"  Chat model: {chat_model}")
    
    assert current_provider == 'openrouter'
    assert deep_research_model == 'perplexity/sonar-deep-research'
    assert chat_model == 'anthropic/claude-3.5-haiku'
    
    print("\n‚úÖ Provider model selection test passed!")


def test_fallback_scenario_simulation():
    """Simulate the error scenario: OpenRouter provider with OpenAI model input"""
    print("\nTesting fallback scenario simulation...")
    
    # Set provider to OpenRouter
    set_current_provider('openrouter')
    
    # Simulate the scenario where a project has an OpenAI model saved
    # but the current provider is OpenRouter
    openai_model = "o4-mini-deep-research-2025-06-26"  # This would cause the error
    openrouter_chat_model = get_model_for_task('chat')  # This should be used in fallback
    
    print(f"Scenario:")
    print(f"  Current provider: {get_current_provider()}")
    print(f"  Project's saved model (OpenAI): {openai_model}")
    print(f"  Expected fallback model (OpenRouter): {openrouter_chat_model}")
    
    # Test the logic that should prevent the error
    # This simulates what our fix does in the fallback logic
    from utils.providers import ProviderError
    current_provider = get_current_provider()
    
    try:
        fallback_model = get_model_for_task("chat", current_provider)
        print(f"  Fallback model selected: {fallback_model}")
        assert fallback_model == openrouter_chat_model
        assert fallback_model != openai_model  # Ensure we're not using the incompatible model
        print("‚úÖ Fallback model selection works correctly!")
    except ProviderError as e:
        print(f"‚ùå Provider error: {e}")
        raise
    
    print("‚úÖ Fallback scenario simulation passed!")


def test_model_compatibility():
    """Test model compatibility across providers"""
    print("\nTesting model compatibility...")
    
    # OpenAI models should not be valid for OpenRouter
    openai_models = ["o4-mini-deep-research-2025-06-26", "gpt-4o-mini"]
    openrouter_models = ["perplexity/sonar-deep-research", "anthropic/claude-3.5-haiku"]
    
    # Test that each provider has its own set of models
    set_current_provider('openai')
    for model in openai_models:
        # These should be found in OpenAI config but not in OpenRouter config
        print(f"  OpenAI model '{model}' - checking if available for OpenAI")
        try:
            # This would be part of validation logic
            from utils.config import get_provider_config
            openai_config = get_provider_config('openai')
            assert model in openai_config.values() or model in openai_config.get('token_limits', {})
            print(f"    ‚úÖ Available for OpenAI")
        except Exception as e:
            print(f"    ‚ùå Error checking OpenAI compatibility: {e}")
    
    set_current_provider('openrouter')
    for model in openrouter_models:
        print(f"  OpenRouter model '{model}' - checking if available for OpenRouter")
        try:
            openrouter_config = get_provider_config('openrouter')
            assert model in openrouter_config.values() or model in openrouter_config.get('token_limits', {})
            print(f"    ‚úÖ Available for OpenRouter")
        except Exception as e:
            print(f"    ‚ùå Error checking OpenRouter compatibility: {e}")
    
    print("‚úÖ Model compatibility test passed!")


if __name__ == "__main__":
    print("=" * 60)
    print("Running Fallback Model Fix Tests")
    print("=" * 60)
    
    try:
        test_fallback_model_selection()
        test_fallback_scenario_simulation()
        test_model_compatibility()
        
        print("\n" + "=" * 60)
        print("üéâ All tests passed! The fallback model fix should work correctly.")
        print("=" * 60)
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {e}")
        print("=" * 60)
        sys.exit(1)